{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Task Learning(MTL) ê°€ì¤‘ì¹˜ ë¹„ìœ¨ ì ìš© ëª¨ë¸\n",
    "í´ë˜ìŠ¤ë³„ ë°ì´í„° ê°œìˆ˜ ë¹„ìœ¨ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì ìš© ë¹„ìœ¨ì„ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ho_e_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ë¼ë²¨ë§\n",
    "- CustomDataset í´ë˜ìŠ¤ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì½ê³ , **íŒŒì¼ëª…ì— í¬í•¨ëœ ì •ë³´**ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¼ë²¨ì„ ì¶”ì¶œ\n",
    "    - '_'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì„¯ ë²ˆì§¸ ê°’ì€ ì§ˆë³‘ ì •ë³´\n",
    "    - '_'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—¬ì„¯ ë²ˆì§¸ ê°’ì€ ì‘ë¬¼ ì •ë³´\n",
    "- ì‘ë¬¼ ë²ˆí˜¸ì™€ ì§ˆë³‘ ë²ˆí˜¸ë¥¼ **ë¼ë²¨ ë§¤í•‘**ì„ í†µí•´ ê°ê° 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "    - ì›ë³¸ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë¼ë²¨ê°’ì´ ê°œìˆ˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ â†’ ë§¤í•‘ ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_map, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_map = label_map  # ë¼ë²¨ ë§µì„ ì¸ìë¡œ ë°›ìŒ\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.transform = transform if transform is not None else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_paths[index]\n",
    "        filename = os.path.basename(path).split('_')\n",
    "\n",
    "        # ë¼ë²¨ ì¶”ì¶œ\n",
    "        disease_label = int(filename[4])  # ë‹¤ì„¯ ë²ˆì§¸ í•­ëª©ì´ ì§ˆë³‘ ë²ˆí˜¸\n",
    "        crop_label = int(filename[5])     # ì—¬ì„¯ ë²ˆì§¸ í•­ëª©ì´ ì‘ë¬¼ ë²ˆí˜¸\n",
    "\n",
    "        # ë¼ë²¨ì„ ë§µí•‘í•˜ì—¬ ì²˜ë¦¬\n",
    "        crop_label = torch.tensor(self.label_map['crop'][crop_label], dtype=torch.long)  \n",
    "        disease_label = torch.tensor(self.label_map['disease'][disease_label], dtype=torch.long)\n",
    "\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, crop_label, disease_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ë²¨ ë§µ ì •ì˜\n",
    "label_map = {\n",
    "    'crop': {1: 0, 2: 1, 3: 2, 6: 3, 9: 4},  # ìˆ«ìì™€ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘\n",
    "    'disease': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 11: 7, 12: 8, 16: 9, 17: 10, 18: 11}\n",
    "}\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "train_dataset = CustomDataset(root_dir='./mtl_dataset_plus/Training', label_map=label_map)\n",
    "val_dataset = CustomDataset(root_dir='./mtl_dataset_plus/Validation', label_map=label_map)\n",
    "test_dataset = CustomDataset(root_dir='./mtl_dataset_plus/Test', label_map=label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ ëª¨ë¸ êµ¬ì¶• - Multi-Task_Learning(MTL) ëª¨ë¸\n",
    "\n",
    "1. **ê³µìœ  ë°±ë³¸(Backbone): ResNet-34**\n",
    "    - ResNet-34: 34ê°œì˜ ê³„ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ì”ì°¨ ë„¤íŠ¸ì›Œí¬(Residual Network)\n",
    "    - ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(`ResNet34_Weights.DEFAULT`)ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”\n",
    "    - ë§ˆì§€ë§‰ Fully Connected(FC) Layerë¥¼ ì œì™¸í•œ ëª¨ë“  ê³„ì¸µì„ ê³µìœ \n",
    "    - ë‹¤ì¤‘ íƒœìŠ¤í¬ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ Fully Connected(FC) Layer, ì¦‰ ë¶„ë¥˜ê¸°ë¥¼ ì œê±°\n",
    "    - ResNet-34ì˜ ë§ˆì§€ë§‰ í’€ë§ ê³„ì¸µì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì€ 512ê°œì˜ ì°¨ì›(feature dimension)ì„ ê°€ì§€ê³  ìˆìŒ\n",
    "\n",
    "2. **íƒœìŠ¤í¬ë³„ ì¶œë ¥ì¸µ(Task-Specific Heads)**\n",
    "    - ì‘ë¬¼ ë¶„ë¥˜ì™€ ì§ˆë³‘ ë¶„ë¥˜ë¥¼ ì§„í–‰\n",
    "    - ê° ë¶„ë¥˜ê¸°ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\n",
    "        1) Crop Head (ì‘ë¬¼ ë¶„ë¥˜ê¸°)\n",
    "            - ì…ë ¥ íŠ¹ì§•ì„ í™œìš©í•˜ì—¬ 5ê°œ ì‘ë¬¼ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜\n",
    "            - **ì¶œë ¥ ë…¸ë“œ: 5ê°œ**\n",
    "            - í™œì„±í™” í•¨ìˆ˜: Softmax í•¨ìˆ˜\n",
    "            - ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss\n",
    "\n",
    "        2) Disease Head (ì§ˆë³‘ ë¶„ë¥˜ê¸°)\n",
    "            - ì…ë ¥ íŠ¹ì§•ì„ í™œìš©í•˜ì—¬ 12ê°œ ì§ˆë³‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜\n",
    "            - **ì¶œë ¥ ë…¸ë“œ: 12ê°œ**\n",
    "            - í™œì„±í™” í•¨ìˆ˜: Softmax í•¨ìˆ˜\n",
    "            - ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì •ì˜\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone, num_crops, num_diseases):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.backbone = backbone  # ResNet-34 backbone\n",
    "        self.n_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()  # ë§ˆì§€ë§‰ fully connected layer(ë¶„ë¥˜ê¸°) ì œê±°\n",
    "        \n",
    "        # íƒœìŠ¤í¬ íŠ¹í™” ë¶„ë¥˜ê¸°\n",
    "        self.crop_head = nn.Linear(self.n_features, num_crops)          # ì‘ë¬¼ í´ë˜ìŠ¤\n",
    "        self.disease_head = nn.Linear(self.n_features, num_diseases)    # ì§ˆë³‘ í´ë˜ìŠ¤\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)  # ê³µìœ  backbone\n",
    "        crop_output = self.crop_head(features)  # ì‘ë¬¼ ë¶„ë¥˜ ì¶œë ¥ê°’\n",
    "        disease_output = self.disease_head(features)  # ì§ˆë³‘ ë¶„ë¥˜ ì¶œë ¥ê°’\n",
    "        return crop_output, disease_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ìƒì„±\n",
    "1. ê³µìœ  ë°±ë³¸\n",
    "    - `resnet34(weights=ResNet34_Weights.DEFAULT)`\n",
    "    - ì‚¬ì „í•™ìŠµëœ ResNet-34 ëª¨ë¸ ì‚¬ìš©\n",
    "2. ëª¨ë¸ ìƒì„±\n",
    "    - `MultiTaskModel(backbone, num_crops, num_diseases).to(device)`\n",
    "    - ë¶„ë¥˜í•  í´ë˜ìŠ¤ ìˆ˜ì™€ ê³µìœ  ë°±ë³¸ì„ ì¸ìˆ˜ë¡œ ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„±\n",
    "num_crops = 5  # ì‘ë¬¼ í´ë˜ìŠ¤ ìˆ˜\n",
    "num_diseases = 12  # ì§ˆë³‘ í´ë˜ìŠ¤ ìˆ˜\n",
    "backbone = resnet34(weights=ResNet34_Weights.DEFAULT)  # ì‚¬ì „í•™ìŠµëœ ResNet-34\n",
    "model = MultiTaskModel(backbone, num_crops, num_diseases).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ë°©ì‹\n",
    "1. **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜**\n",
    "    - **í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°**\n",
    "        - í•™ìŠµ ë°ì´í„°ì˜ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ê°€ ë¶ˆê· í˜•í•˜ë¯€ë¡œ, ì´ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©\n",
    "        - ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ì— ì ìš©ë˜ì–´ ìƒ˜í”Œì´ ì ì€ í´ë˜ìŠ¤ì— ë” í° ì˜í–¥ì„ ë¶€ì—¬í•¨\n",
    "        1) í•™ìŠµ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©°, ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ê³„ì‚°\n",
    "        2) í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ì˜ ì—­ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°\n",
    "    - ê° íƒœìŠ¤í¬ì˜ ì†ì‹¤ í•¨ìˆ˜ëŠ” CrossEntropyLossë¥¼ ì‚¬ìš©í•˜ë˜ ê° íƒœìŠ¤í¬ì— ë§ëŠ” ê°€ì¤‘ì¹˜ ì ìš©(`crop_class_weights`,`disease_class_weights`)\n",
    "    - ë‹¤ì¤‘ íƒœìŠ¤í¬ í•™ìŠµì˜ ì´ ì†ì‹¤ì€ ë‘ íƒœìŠ¤í¬ì˜ ì†ì‹¤ì„ ë™ì¼í•œ ë¹„ìœ¨(0.5)ë¡œ ê°€ì¤‘í•©í•˜ì—¬ ê³„ì‚°\n",
    "    - **`TotalÂ Loss = 0.5 Ã— CropÂ Loss + 0.5 Ã— DiseaseÂ Loss`**\n",
    "        - ì‘ë¬¼ ë¶„ë¥˜ì™€ ì§ˆë³‘ ë¶„ë¥˜ì˜ ì†ì‹¤ì„ *0.3:0.7* ë¹„ìœ¨ë¡œ ì‹¤í—˜ì„ í•´ë´¤ìœ¼ë‚˜ **ì „ë°˜ì ìœ¼ë¡œ ë™ì¼í•œ ë¹„ìœ¨(0.5)ë¡œ ê°€ì¤‘í•©í•˜ì˜€ì„ ë•Œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„**\n",
    "\n",
    "\n",
    "2. **ìµœì í™” ì•Œê³ ë¦¬ì¦˜**\n",
    "    - Adam Optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸\n",
    "    - í•™ìŠµë¥ ì€ 0.001ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ ê³„ì‚°\n",
    "crop_counts = Counter()\n",
    "disease_counts = Counter()\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìˆœíšŒí•˜ë©° ë¼ë²¨ ìˆ˜ì§‘\n",
    "for i in range(len(train_dataset)):\n",
    "    _, crop_label, disease_label = train_dataset[i]  # __getitem__ í˜¸ì¶œ\n",
    "    crop_counts[crop_label] += 1\n",
    "    disease_counts[disease_label] += 1\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(\"Crop class counts:\", dict(crop_counts))\n",
    "print(\"Disease class counts:\", dict(disease_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "total_crop_samples = sum(crop_counts.values())\n",
    "total_disease_samples = sum(disease_counts.values())\n",
    "\n",
    "crop_class_weights = torch.tensor(\n",
    "    [total_crop_samples / crop_counts.get(c, 1) for c in range(num_crops)],\n",
    "    dtype=torch.float,\n",
    ").to(device)\n",
    "\n",
    "disease_class_weights = torch.tensor(\n",
    "    [total_disease_samples / disease_counts.get(d, 1) for d in range(num_diseases)],\n",
    "    dtype=torch.float,\n",
    ").to(device)\n",
    "\n",
    "print(\"Crop class weights:\", crop_class_weights)\n",
    "print(\"Disease class weights:\", disease_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° íƒœìŠ¤í¬ë³„ë¡œ ê°€ì¤‘ì¹˜ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "criterion_crop = nn.CrossEntropyLoss(weight=crop_class_weights)\n",
    "criterion_disease = nn.CrossEntropyLoss(weight=disease_class_weights)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì •ì˜\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard ì„¤ì •\n",
    "writer = SummaryWriter('./runs_mtl/weight_mtl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ë£¨í”„\n",
    "best_val_disease_acc = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ====== Training ======\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_crop = 0\n",
    "    correct_disease = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, crop_labels, disease_labels in train_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        crop_labels = crop_labels.to(device, non_blocking=True)\n",
    "        disease_labels = disease_labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Optimizer ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward ë° ì†ì‹¤ ê³„ì‚°\n",
    "        crop_outputs, disease_outputs = model(images)\n",
    "        crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "        disease_loss = criterion_disease(disease_outputs, disease_labels)\n",
    "        total_loss = 0.5 * crop_loss + 0.5 * disease_loss\n",
    "\n",
    "        # Backward ë° Optimizer ì—…ë°ì´íŠ¸\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°\n",
    "        running_loss += total_loss.item()\n",
    "        _, crop_predicted = torch.max(crop_outputs, 1)\n",
    "        _, disease_predicted = torch.max(disease_outputs, 1)\n",
    "        total += crop_labels.size(0)\n",
    "        correct_crop += (crop_predicted == crop_labels).sum().item()\n",
    "        correct_disease += (disease_predicted == disease_labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_crop_acc = 100 * correct_crop / total\n",
    "    train_disease_acc = 100 * correct_disease / total\n",
    "\n",
    "    # ====== Validation ======\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_crop = 0\n",
    "    correct_disease = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, crop_labels, disease_labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            crop_labels = crop_labels.to(device, non_blocking=True)\n",
    "            disease_labels = disease_labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward ë° ì†ì‹¤ ê³„ì‚°\n",
    "            crop_outputs, disease_outputs = model(images)\n",
    "            crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "            disease_loss = criterion_disease(disease_outputs, disease_labels)\n",
    "            val_loss += (0.5 * crop_loss + 0.5 * disease_loss).item()\n",
    "\n",
    "            # ì •í™•ë„ ê³„ì‚°\n",
    "            _, crop_predicted = torch.max(crop_outputs, 1)\n",
    "            _, disease_predicted = torch.max(disease_outputs, 1)\n",
    "            total += crop_labels.size(0)\n",
    "            correct_crop += (crop_predicted == crop_labels).sum().item()\n",
    "            correct_disease += (disease_predicted == disease_labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_crop_acc = 100 * correct_crop / total\n",
    "    val_disease_acc = 100 * correct_disease / total\n",
    "\n",
    "    # TensorBoardì— ê¸°ë¡\n",
    "    writer.add_scalar('Train/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Train/Crop_Accuracy', train_crop_acc, epoch)\n",
    "    writer.add_scalar('Train/Disease_Accuracy', train_disease_acc, epoch)\n",
    "    writer.add_scalar('Validation/Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Validation/Crop_Accuracy', val_crop_acc, epoch)\n",
    "    writer.add_scalar('Validation/Disease_Accuracy', val_disease_acc, epoch)\n",
    "\n",
    "    # ì¶œë ¥\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Crop Acc: {train_crop_acc:.2f}%, Train Disease Acc: {train_disease_acc:.2f}%, '\n",
    "            f'Val Loss: {val_loss:.4f}, Val Crop Acc: {val_crop_acc:.2f}%, Val Disease Acc: {val_disease_acc:.2f}%')\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    if val_disease_acc > best_val_disease_acc:\n",
    "        best_val_disease_acc = val_disease_acc\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "torch.save(best_model, 'best_weight_mtl_model.pth')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í‰ê°€\n",
    "model.load_state_dict(torch.load('best_weight_mtl_model.pth', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "correct_crop = 0\n",
    "correct_disease = 0\n",
    "total = 0\n",
    "\n",
    "pred_crop_labels = []\n",
    "pred_disease_labels = []\n",
    "true_crop_labels = []\n",
    "true_disease_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, crop_labels, disease_labels in test_loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        crop_labels = crop_labels.to(device, non_blocking=True)\n",
    "        disease_labels = disease_labels.to(device, non_blocking=True)\n",
    "\n",
    "        crop_outputs, disease_outputs = model(images)\n",
    "        _, crop_predicted = torch.max(crop_outputs, 1)\n",
    "        _, disease_predicted = torch.max(disease_outputs, 1)\n",
    "\n",
    "        total += crop_labels.size(0)\n",
    "        correct_crop += (crop_predicted == crop_labels).sum().item()\n",
    "        correct_disease += (disease_predicted == disease_labels).sum().item()\n",
    "\n",
    "        pred_crop_labels.extend(crop_predicted.cpu().numpy())\n",
    "        pred_disease_labels.extend(disease_predicted.cpu().numpy())\n",
    "        true_crop_labels.extend(crop_labels.cpu().numpy())\n",
    "        true_disease_labels.extend(disease_labels.cpu().numpy())\n",
    "\n",
    "test_crop_acc = 100 * correct_crop / total\n",
    "test_disease_acc = 100 * correct_disease / total\n",
    "\n",
    "print(f\"Test Crop Accuracy: {test_crop_acc:.2f}%\")\n",
    "print(f\"Test Disease Accuracy: {test_disease_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜¼ë™ í–‰ë ¬ ì‹œê°í™” í•¨ìˆ˜\n",
    "def plot_confusion_matrix(true_labels, pred_labels, classes, title, save_path=None):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'  # ë§‘ì€ê³ ë”•\n",
    "    plt.rcParams['axes.unicode_minus'] = False   # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
    "plot_confusion_matrix(\n",
    "    true_crop_labels, \n",
    "    pred_crop_labels, \n",
    "    classes=['ê³ ì¶”', 'ë¬´', 'ë°°ì¶”', 'ì˜¤ì´', 'íŒŒ'], \n",
    "    title=\"Crop Classification Confusion Matrix\",\n",
    "    save_path='./basic_mtl/crop_confusion_matrix.png'\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    true_disease_labels, \n",
    "    pred_disease_labels, \n",
    "    classes=['ì •ìƒ', 'ê³ ì¶”íƒ„ì €ë³‘', 'ê³ ì¶”í°ê°€ë£¨ë³‘', 'ë¬´ê²€ì€ë¬´ëŠ¬ë³‘', 'ë¬´ë…¸ê· ë³‘', 'ë°°ì¶”ê²€ìŒì©ìŒë³‘', 'ë°°ì¶”ë…¸ê· ë³‘', 'ì˜¤ì´ë…¸ê· ë³‘', 'ì˜¤ì´í°ê°€ë£¨ë³‘', 'íŒŒê²€ì€ë¬´ëŠ¬ë³‘', 'íŒŒë…¸ê· ë³‘', 'íŒŒë…¹ë³‘'], \n",
    "    title=\"Disease Classification Confusion Matrix\",\n",
    "    save_path='./basic_mtl/disease_confusion_matrix.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6021 (pid 26732), started 0:00:12 ago. (Use '!kill 26732' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9fa8504ba4916e00\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9fa8504ba4916e00\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6021;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard ì‹¤í–‰ (ì£¼ì„ ì²˜ë¦¬ëœ ìƒíƒœ)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir './runs_mtl/' --port 6025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
